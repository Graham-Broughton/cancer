{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1TxvnyrCf7Pm6cDX1Z_lBb8uOvgzZLIej","authorship_tag":"ABX9TyPCSAIO58CPI4798Og4QFx9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"L8lx7pZfAsoj","executionInfo":{"status":"ok","timestamp":1675592966525,"user_tz":480,"elapsed":7217,"user":{"displayName":"graham broughton","userId":"15728648374086258761"}},"outputId":"4fa1cfc2-8855-4f7a-91ec-08320aa23bba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting dicomsdl\n","  Downloading dicomsdl-0.109.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pylibjpeg\n","  Downloading pylibjpeg-1.4.0-py3-none-any.whl (28 kB)\n","Collecting keras_cv_attention_models\n","  Downloading keras_cv_attention_models-1.3.9-py3-none-any.whl (572 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.5/572.5 KB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydicom\n","  Downloading pydicom-2.3.1-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-dotenv\n","  Downloading python_dotenv-0.21.1-py3-none-any.whl (19 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pylibjpeg) (1.21.6)\n","Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.8/dist-packages (from keras_cv_attention_models) (4.8.2)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (from keras_cv_attention_models) (2.9.2)\n","Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (15.0.6.1)\n","Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (2.9.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (1.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (57.4.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (1.15.0)\n","Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (1.12)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (4.4.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (1.51.1)\n","Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (2.9.1)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (1.1.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (2.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (3.3.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (0.2.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (3.19.6)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (1.6.3)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (3.1.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (0.4.0)\n","Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (2.9.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (1.14.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (23.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_cv_attention_models) (0.30.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons->keras_cv_attention_models) (2.7.1)\n","Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv_attention_models) (1.0.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv_attention_models) (0.3.6)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv_attention_models) (5.4.8)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv_attention_models) (0.1.8)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv_attention_models) (2.25.1)\n","Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv_attention_models) (0.10.2)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv_attention_models) (5.10.2)\n","Requirement already satisfied: promise in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv_attention_models) (2.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv_attention_models) (7.1.2)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv_attention_models) (1.12.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from tensorflow-datasets->keras_cv_attention_models) (4.64.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow->keras_cv_attention_models) (0.38.4)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.8/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->keras_cv_attention_models) (3.12.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (1.24.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras_cv_attention_models) (2022.12.7)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_cv_attention_models) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_cv_attention_models) (0.4.6)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_cv_attention_models) (1.0.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_cv_attention_models) (2.16.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_cv_attention_models) (3.4.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_cv_attention_models) (0.6.1)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras_cv_attention_models) (1.58.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras_cv_attention_models) (5.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras_cv_attention_models) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras_cv_attention_models) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras_cv_attention_models) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->keras_cv_attention_models) (6.0.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras_cv_attention_models) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras_cv_attention_models) (3.2.2)\n","Installing collected packages: tensorflow-addons, python-dotenv, pylibjpeg, pydicom, dicomsdl, keras_cv_attention_models\n","Successfully installed dicomsdl-0.109.1 keras_cv_attention_models-1.3.9 pydicom-2.3.1 pylibjpeg-1.4.0 python-dotenv-0.21.1 tensorflow-addons-0.19.0\n"]}],"source":["!pip install dicomsdl pylibjpeg keras_cv_attention_models pydicom python-dotenv"]},{"cell_type":"code","source":["%cd drive/MyDrive/cancer\n","import numpy as np\n","import pandas as pd\n","import pylibjpeg\n","import pydicom\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","from joblib import Parallel, delayed\n","from tqdm.notebook import tqdm\n","from multiprocessing import cpu_count\n","from keras_cv_attention_models import convnext\n","\n","import cv2\n","import glob\n","import importlib\n","import os\n","import joblib\n","import time\n","import dicomsdl\n","import gc\n","import dotenv\n","import sys\n","sys.path.append('..')\n","from config import CFG\n","CFG = CFG()\n","\n","# Tensorflow and CV2 set number of threads to 1 for speedup in parallell function mapping\n","tf.config.threading.set_inter_op_parallelism_threads(num_threads=1)\n","cv2.setNumThreads(1)\n","\n","# Pandas DataFrame Display Options\n","pd.options.display.max_colwidth = 99"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0t3m-FN4H4PL","executionInfo":{"status":"ok","timestamp":1675593529923,"user_tz":480,"elapsed":4785,"user":{"displayName":"graham broughton","userId":"15728648374086258761"}},"outputId":"eb93d999-9e86-41ac-d635-30f8ed218dea"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/cancer\n"]}]},{"cell_type":"code","source":["IS_INTERACTIVE = False\n","\n","TARGET_HEIGHT = CFG.IMG_HEIGHT\n","TARGET_WIDTH = CFG.IMG_WIDTH\n","N_CHANNELS = 1\n","INPUT_SHAPE = (TARGET_HEIGHT, TARGET_WIDTH, N_CHANNELS)\n","TARGET_HEIGHT_WIDTH_RATIO = TARGET_HEIGHT / TARGET_WIDTH\n","THRESHOLD_BEST = 0.50\n","\n","CLAHE = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(32, 32))\n","\n","CROP_IMAGE = True\n","APPLY_CLAHE = False\n","APPLY_EQ_HIST = False\n","\n","IMAGE_FORMAT = 'jpg'"],"metadata":{"id":"-CuoMQ8EKCYc","executionInfo":{"status":"ok","timestamp":1675593600582,"user_tz":480,"elapsed":162,"user":{"displayName":"graham broughton","userId":"15728648374086258761"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Source: https://www.kaggle.com/code/bobdegraaf/dicomsdl-voi-lut\n","def voi_lut(image, dicom):\n","    # Additional Checks\n","    if 'WindowWidth' not in dicom.getPixelDataInfo() or 'WindowWidth' not in dicom.getPixelDataInfo():\n","        return image\n","    \n","    # Load only the variables we need\n","    center = dicom['WindowCenter']\n","    width = dicom['WindowWidth']\n","    bits_stored = dicom['BitsStored']\n","    voi_lut_function = dicom['VOILUTFunction']\n","\n","    # For sigmoid it's a list, otherwise a single value\n","    if isinstance(center, list):\n","        center = center[0]\n","    if isinstance(width, list):\n","        width = width[0]\n","\n","    # Set y_min, max & range\n","    y_min = 0\n","    y_max = float(2**bits_stored - 1)\n","    y_range = y_max\n","\n","    # Function with default LINEAR (so for Nan, it will use linear)\n","    if voi_lut_function == 'SIGMOID':\n","        image = y_range / (1 + np.exp(-4 * (image - center) / width)) + y_min\n","    else:\n","        # Checks width for < 1 (in our case not necessary, always >= 750)\n","        center -= 0.5\n","        width -= 1\n","\n","        below = image <= (center - width / 2)\n","        above = image > (center + width / 2)\n","        between = np.logical_and(~below, ~above)\n","\n","        image[below] = y_min\n","        image[above] = y_max\n","        if between.any():\n","            image[between] = (\n","                ((image[between] - center) / width + 0.5) * y_range + y_min\n","            )\n","\n","    return image\n","\n","# Smooth vector used to smoothen sums/stds of axes\n","def smooth(l):\n","    # kernel size is 1% of vector\n","    kernel_size = int(len(l) * 0.01)\n","    kernel = np.ones(kernel_size) / kernel_size\n","    return np.convolve(l, kernel, mode='same')\n","\n","# X Crop offset based on first column with sum below 5% of maximum column sums*std\n","def get_x_offset(image, max_col_sum_ratio_threshold=0.05, debug=None):\n","    # Image Dimensions\n","    H, W = image.shape\n","    # Percentual margin added to offset\n","    margin = int(image.shape[1] * 0.00)\n","    # Threshold values based on smoothed sum x std to capture varying intensity columns\n","    vv = smooth(image.sum(axis=0).squeeze()) * smooth(image.std(axis=0).squeeze())\n","    # Find maximum sum in first 75% of columns\n","    vv_argmax = vv[:int(image.shape[1] * 0.75)].argmax()\n","    # Threshold value\n","    vv_threshold = vv.max() * max_col_sum_ratio_threshold\n","    \n","    # Find first column after maximum column below threshold value\n","    for offset, v in enumerate(vv):\n","        # Start searching from vv_argmax\n","        if offset < vv_argmax:\n","            continue\n","        \n","        # Column below threshold value found\n","        if v < vv_threshold:\n","            offset = min(W, offset + margin)\n","            break\n","            \n","    if isinstance(debug, np.ndarray):\n","        debug[1].imshow(image)\n","        debug[1].set_title('X Offset')\n","        vv_scale = H / vv.max() * 0.90\n","        # Values\n","        debug[1].plot(H - vv * vv_scale , c='red', label='vv')\n","        # Threshold\n","        debug[1].hlines(H - vv_threshold * vv_scale, 0, W -1, colors='orange', label='threshold')\n","        # Max Value\n","        debug[1].scatter(vv_argmax, H - vv[vv_argmax] * vv_scale, c='blue', s=100, label='Max', zorder=np.PINF)\n","        # First Column Below Threshold\n","        debug[1].scatter(offset, H - vv[offset] * vv_scale, c='purple', s=100, label='Offset', zorder=np.PINF)\n","        debug[1].set_ylim(H, 0)\n","        debug[1].legend()\n","        debug[1].axis('off')\n","        \n","    return offset\n","\n","# Y Crop offset based on first bottom and top rows with sum below 10% of maximum row sum*std\n","def get_y_offsets(image, max_row_sum_ratio_threshold=0.10, debug=None):\n","    # Image Dimensions\n","    H, W = image.shape\n","    # Margin to add to offsets\n","    margin = 0\n","    # Threshold values based on smoothed sum x std to capture varying intensity columns\n","    vv = smooth(image.sum(axis=1).squeeze()) * smooth(image.std(axis=1).squeeze())\n","    # Find maximum sum * std row in inter quartile rows\n","    vv_argmax = int(image.shape[0] * 0.25) + vv[int(image.shape[0] * 0.25):int(image.shape[0] * 0.75)].argmax()\n","    # Threshold value\n","    vv_threshold = vv.max() * max_row_sum_ratio_threshold\n","    # Default crop offsets\n","    offset_bottom = 0\n","    offset_top = H\n","\n","    # Bottom offset, search from argmax to bottom\n","    for offset in reversed(range(0, vv_argmax)):\n","        v = vv[offset]\n","        if v < vv_threshold:\n","            offset_bottom = offset\n","            break\n","    \n","    if isinstance(debug, np.ndarray):\n","        debug[2].imshow(image)\n","        debug[2].set_title('Y Bottom Offset')\n","        vv_scale = W / vv.max() * 0.90\n","        # Values\n","        debug[2].plot(vv * vv_scale, np.arange(H), c='red', label='vv')\n","        # Threshold\n","        debug[2].vlines(vv_threshold * vv_scale, 0, H -1, colors='orange', label='threshold')\n","        # Max Value\n","        debug[2].scatter(vv[vv_argmax] * vv_scale, vv_argmax, c='blue', s=100, label='Max', zorder=np.PINF)\n","        # First Column Below Threshold\n","        debug[2].scatter(vv[offset_bottom] * vv_scale, offset_bottom, c='purple', s=100, label='Offset', zorder=np.PINF)\n","        debug[2].set_ylim(H, 0)\n","        debug[2].legend()\n","        debug[2].axis('off')\n","            \n","    # Top offset, search from argmax to top\n","    for offset in range(vv_argmax, H):\n","        v = vv[offset]\n","        if v < vv_threshold:\n","            offset_top = offset\n","            break\n","            \n","    if isinstance(debug, np.ndarray):\n","        debug[3].imshow(image)\n","        debug[3].set_title('Y Top Offset')\n","        vv_scale = W / vv.max() * 0.90\n","        # Values\n","        debug[3].plot(vv * vv_scale, np.arange(H) , c='red', label='vv')\n","        # Threshold\n","        debug[3].vlines(vv_threshold * vv_scale, 0, H -1, colors='orange', label='threshold')\n","        # Max Value\n","        debug[3].scatter(vv[vv_argmax] * vv_scale, vv_argmax, c='blue', s=100, label='Max', zorder=np.PINF)\n","        # First Column Below Threshold\n","        debug[3].scatter(vv[offset_top] * vv_scale, offset_top, c='purple', s=100, label='Offset', zorder=np.PINF)\n","        debug[2].set_ylim(H, 0)\n","        debug[3].legend()\n","        debug[3].axis('off')\n","            \n","    return max(0, offset_bottom - margin), min(image.shape[0], offset_top + margin)\n","\n","# Crop image and pad offsets to target image height/width ratio to preserve information\n","def crop(image, size=None, debug=False):\n","    # Image dimensions\n","    H, W = image.shape\n","    # Compute x/bottom/top offsets\n","    x_offset = get_x_offset(image, debug=debug)\n","    offset_bottom, offset_top = get_y_offsets(image[:,:x_offset], debug=debug)\n","    # Crop Height and Width\n","    h_crop = offset_top - offset_bottom\n","    w_crop = x_offset\n","    \n","    # Pad crop offsets to target aspect ratio\n","    if size is not None:\n","        # Height too large, pad x offset\n","        if (h_crop / w_crop) > TARGET_HEIGHT_WIDTH_RATIO:\n","            x_offset += int(h_crop / TARGET_HEIGHT_WIDTH_RATIO - w_crop)\n","        else:\n","            # Height too small, pad bottom/top offsets\n","            offset_bottom -= int(0.50 * (w_crop * TARGET_HEIGHT_WIDTH_RATIO - h_crop))\n","            offset_bottom_correction = max(0, -offset_bottom)\n","            offset_bottom += offset_bottom_correction\n","\n","            offset_top += int(0.50 * (w_crop * TARGET_HEIGHT_WIDTH_RATIO - h_crop))\n","            offset_top += offset_bottom_correction\n","        \n","    # Crop Image\n","    image = image[offset_bottom:offset_top:,:x_offset]\n","        \n","    return image\n","\n","def process(file_path, size=(TARGET_WIDTH, TARGET_HEIGHT), crop_image=CROP_IMAGE, apply_clahe=APPLY_CLAHE, apply_eq_hist=APPLY_EQ_HIST, debug=False, save=True):\n","    # Read Dicom File\n","    dicom = dicomsdl.open(file_path)\n","    image = dicom.pixelData()\n","    \n","    # Save original image for debug purposes\n","    if debug:\n","        fig, axes = plt.subplots(1, 5, figsize=(20,10))\n","        image0 = np.copy(image)\n","        axes[0].imshow(image0)\n","        axes[0].set_title('Original Image')\n","        axes[0].axis('off')\n","    else:\n","        axes = False\n","    \n","    # voi_lut\n","    try:\n","        image = voi_lut(image, dicom)\n","    except:\n","        pass\n","    \n","    # Some images have 0 values as highest intensity and need to be inverted\n","    if dicom.getPixelDataInfo()['PhotometricInterpretation'] == 'MONOCHROME1':\n","        image = np.max(image) - image\n","\n","    # Normalize [0,1] range\n","    image = (image - image.min()) / (image.max() - image.min())\n","\n","    # Convert to uint8 image in range [0, 255]\n","    image = (image * 255).astype(np.uint8)\n","    \n","    # Flip T0 Left/Right Orientation\n","    h0, w0 = image.shape\n","    if image[:,int(-w0 * 0.10):].sum() > image[:,:int(w0 * 0.10)].sum():\n","        image = np.flip(image, axis=1)\n","    \n","    # Crop Image\n","    if crop_image:\n","        image = crop(image, debug=axes)\n","        \n","    # Resize\n","    if size is not None:\n","        # Pad black pixels to make square image\n","        h, w = image.shape\n","        if (h / w) > TARGET_HEIGHT_WIDTH_RATIO:\n","            pad = int(h / TARGET_HEIGHT_WIDTH_RATIO - w)\n","            image = np.pad(image, [[0,0], [0, pad]])\n","            h, w = image.shape\n","        else:\n","            pad = int(0.50 * (w * TARGET_HEIGHT_WIDTH_RATIO - h))\n","            image = np.pad(image, [[pad, pad], [0,0]])\n","            h, w = image.shape\n","        # Resize\n","        image = cv2.resize(image, size, interpolation=cv2.INTER_AREA)\n","        \n","    # Apply CLAHE contrast enhancement\n","    if apply_clahe:\n","        image = CLAHE.apply(image)\n","        \n","     # Apply Histogram Equalization\n","    if apply_eq_hist:\n","        image = cv2.equalizeHist(image)\n","        \n","    # Show Processed Image    \n","    if debug:\n","        axes[4].imshow(image)\n","        axes[4].set_title('Processed Image')\n","        axes[4].axis('off')\n","        plt.show()\n","        \n","    # Save Only\n","    if save:\n","        image_id = file_path.split('/')[-1].split('.')[0]\n","        if IMAGE_FORMAT == 'png':\n","            cv2.imwrite(f'{image_id}.png', image)\n","        else:\n","            cv2.imwrite(f'{image_id}.jpg', image, [cv2.IMWRITE_JPEG_QUALITY, 95])"],"metadata":{"id":"tEBGUmKQKUwx","executionInfo":{"status":"ok","timestamp":1675593760716,"user_tz":480,"elapsed":204,"user":{"displayName":"graham broughton","userId":"15728648374086258761"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["train = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/train.csv')\n","    \n","def get_file_path(args):\n","    patient_id, image_id = args\n","    return f'/kaggle/input/rsna-breast-cancer-detection/train_images/{patient_id}/{image_id}.dcm'\n","    \n","train['file_path'] = train[['patient_id', 'image_id']].apply(get_file_path, axis=1)"],"metadata":{"id":"7t30uYNVK70J"},"execution_count":null,"outputs":[]}]}